{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "overview_section"
      },
      "source": [
        "# LLM Auto-Prompt Tuning for Code Generation\n",
        "\n",
        "This notebook demonstrates the applications of prompt-tuning Large Language Models (LLMs) with a focus on code generation. It serves as both an educational tool and a practical guide, walking you through various stages from basic model usage to advanced training techniques.\n",
        "\n",
        "## Overview\n",
        "\n",
        "The notebook is organized into five main stages:\n",
        "\n",
        "1. **Code Generation with Base Prompt**: Demonstrate basic code completion using a general-purpose model\n",
        "2. **Data Preparation**: Generate code to download relevant datasets for LLM training\n",
        "3. **Training Data Preprocessing**: Generate code to preprocess data for LLM training\n",
        "4. **Run Auto-Prompt Tuning job to get optimized prompt**: Fetch optimized prompt using base prompt and prompt tuning dataset\n",
        "\n",
        "Each stage builds upon the previous one, providing a comprehensive understanding of the LLM training process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup_section"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Let's start by installing the necessary dependencies and setting up our environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_dependencies"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install -q transformers datasets accelerate torch evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import_libraries"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from datasets import load_dataset\n",
        "import time\n",
        "import json\n",
        "from IPython.display import display, HTML, Markdown"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Stage 1: Demonstrate basic code completion using a general-purpose model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.1 Core Utility Functions\n",
        "\n",
        "***Use the following prompt in the next code cell to generate code***\n",
        "\n",
        "```bash\n",
        "Create Python utility functions for working with language models:\n",
        "\n",
        "1. A 'tokenize' function that takes text and a tokenizer, and returns tokenized text\n",
        "2. A 'stream' function that generates tokens one by one from a model given a prompt\n",
        "3. A 'generate' function that produces complete outputs with configurable parameters\n",
        "\n",
        "Include proper error handling, type hints, and docstrings. Make the functions compatible with Hugging Face Transformers library.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.2 Code generation task demonstration\n",
        "\n",
        "***Use the following prompt in the next code cell to generate code***\n",
        "\n",
        "```bash\n",
        "Write code to demonstrate a simple code generation task using a general-purpose language model with bedrock call using AWS credentials(AWS_ACCESS_KEY_ID,AWS_SECRET_ACCESS_KEY,AWS_SESSION_TOKEN) stored in .env file. The example should:\n",
        "\n",
        "1. Define a programming problem (like implementing a sorting algorithm or data structure)\n",
        "2. Show how to format the prompt for code generation\n",
        "3. Display the generated code\n",
        "\n",
        "Use the utility functions created earlier for model interaction.\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Stage 2: Generate code to prepare datasets for LLM training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.1 Preparing custom data from web scraping and Visualization\n",
        "\n",
        "***Use the following prompt in the next code cell to generate code***\n",
        "\n",
        "```bash\n",
        "Implement code for acquiring custom data for LLM training. The implementation should:\n",
        "\n",
        "1. Demonstrate data acqisition throug web scraping. Here are the list of urls, you can use for this: \n",
        "\"https://raw.githubusercontent.com/TheAlgorithms/Python/master/searches/double_linear_search_recursion.py\", \"https://raw.githubusercontent.com/KosingZhu/tensorflow/master/tensorflow/python/tools/module_util.py\", \"https://raw.githubusercontent.com/EricRemmerswaal/tensorflow/master/tensorflow/python/distribute/distribute_coordinator_context.py\", \"https://raw.githubusercontent.com/computationalartist/tensorflow/master/tensorflow/python/ops/numpy_ops/integration_test/benchmarks/numpy_mlp.py\", \"https://raw.githubusercontent.com/Van-an/tensorflow/master/tensorflow/python/distribute/coordinator/values.py\", \"https://raw.githubusercontent.com/nkgwer/tensorflow/master/tensorflow/lite/tools/visualize.py\", \"https://raw.githubusercontent.com/gitblazer/youtube-dl/master/youtube_dl/version.py\", \"https://raw.githubusercontent.com/Joshua-Barawa/My-Photos/master/venv/lib/python3.8/site-packages/django/contrib/messages/__init__.py\", \"https://raw.githubusercontent.com/PaliC/pytorch/master/test/fx/test_subgraph_rewriter.pyâ€œ\n",
        "2. Include functions for data extraction and initial formatting\n",
        "3. Show how to sample and inspect the acquired data\n",
        "4. Demonstrate saving the data in an appropriate format for further processing\n",
        "\n",
        "Ensure the code is well-documented and includes examples of usage.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.2 Loading existing dataset for LLM training\n",
        "\n",
        "***Use the following prompt in the next code cell to generate code***\n",
        "\n",
        "```bash\n",
        "Create python script to download a dataset from Hugging Face and save it as a CSV.\n",
        "1. Use the dotenv library to load an environment variable named \"HF_ACCESS_TOKEN\".\n",
        "2. Download the 'train' split of the \"Jayveersinh-Raj/prompt-tuning-text-to-code\" dataset. Use the token for authentication.\n",
        "3. Create a 'data' directory if it doesn't exist.\n",
        "4. Save the dataset into the 'data' directory as \"prompt_tuning_train.csv\". Do not include the DataFrame index.\n",
        "5. Print a success message with the final file path.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Stage 3: Generate code to preprocess data for LLM training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.1 Format the raw data into a structured training prompt.\n",
        "\n",
        "***Use the following prompt in the next code cell to generate code***\n",
        "\n",
        "```bash\n",
        "Goal: Format the raw data into a structured training prompt.\n",
        "\n",
        "1. Import pandas to read the CSV file.\n",
        "2. Load the \"data/prompt_tuning_train.csv\" into a pandas DataFrame.\n",
        "3. Display the first 5 rows of the DataFrame to inspect the columns.\n",
        "\n",
        "    (The columns should be 'bad_prompts' and 'improved_prompts')\n",
        "\n",
        "1. Define a function create_training_prompt(row) that takes a row of the DataFrame as input.\n",
        "2. Inside the function, format the data into a clear structure:\n",
        "    1. Start with a system instruction: \"Rewrite the user's initial request into a refined prompt.\"\n",
        "    2. Add a \"### Initial Request:\" section using the value from the 'bad_prompts' column.\n",
        "    3. Add a \"### Refined Prompt:\" section using the value from the 'improved_prompts' column.\n",
        "3. The function should return the complete formatted string.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.2 Create processed json lines format\n",
        "\n",
        "***Use the following prompt in the next code cell to generate code***\n",
        "\n",
        "```bash\n",
        "Now apply the formatting function to the DataFrame.\n",
        "1. Create a new column 'formatted_prompt' in the DataFrame.\n",
        "2. Apply the `create_training_prompt` function to each row of the DataFrame to populate the new column.\n",
        "3. Print the content of the 'formatted_prompt' column for the first row to verify the output.\n",
        "4. Save the processed DataFrame with the new column to \"data/prompt_tuning_train_processed.jsonl\" in JSON Lines format, with orient='records' and lines=True.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "next_steps_section"
      },
      "source": [
        "## Next Steps\n",
        "\n",
        "Now that we have set up the notebook environment and implemented the core utility functions, we can proceed to the next tasks:\n",
        "\n",
        "1. Show the limitations of basic prompts for code generation\n",
        "2. Compare with optimized prompts\n",
        "3. Analyze the differences in performance\n",
        "\n",
        "These will be implemented in the subsequent sections of this notebook."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
